{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "doc_summ.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3U_XOnVM5BI"
      },
      "outputs": [],
      "source": [
        "!!unzip \\*.zip && rm \\*.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "id": "iczu4EeiNO-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install docx2txt"
      ],
      "metadata": {
        "id": "9-k8tnTxNTT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import PyPDF2\n",
        "import docx2txt\n",
        "import sys"
      ],
      "metadata": {
        "id": "pqAYxvLBNTi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "MZYlcNdgNTuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx"
      ],
      "metadata": {
        "id": "h3k8zAmrNT3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.punkt import PunktSentenceTokenizer"
      ],
      "metadata": {
        "id": "MORmdBOENSge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer"
      ],
      "metadata": {
        "id": "6TrWu9yUNgBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we are going to show an example of how the method is working\n",
        "# first let's take the document as an input\n",
        "def readDoc():\n",
        "    name = input('Please input a file name: ') \n",
        "    print('You have asked for the document {}'.format(name))\n",
        "\n",
        "    # now read the type of document\n",
        "    if name.lower().endswith('.txt'):\n",
        "        choice = 1\n",
        "    elif name.lower().endswith('.pdf'):\n",
        "        choice = 2\n",
        "    else:\n",
        "        choice = 3\n",
        "        # print(name)\n",
        "    print(choice)\n",
        "    # Case 1: if it is a .txt file\n",
        "        \n",
        "    if choice == 1:\n",
        "        f = open(name, 'r')\n",
        "        document = f.read()\n",
        "        f.close()\n",
        "            \n",
        "    # Case 2: if it is a .pdf file\n",
        "    elif choice == 2:\n",
        "        pdfFileObj = open(name, 'rb')\n",
        "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
        "        pageObj = pdfReader.getPage(0)\n",
        "        document = pageObj.extractText()\n",
        "        pdfFileObj.close()\n",
        "    \n",
        "    # Case 3: none of the format\n",
        "    else:\n",
        "        print('Failed to load a valid file')\n",
        "        print('Returning an empty string')\n",
        "        document = ''\n",
        "    \n",
        "    print(type(document))\n",
        "    return document"
      ],
      "metadata": {
        "id": "O7gAT8eeNgHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the function used for tokenizing the sentences\n",
        "# tokenization of a sentence: '''provided in cell() above'''\n",
        "\n",
        "def tokenize(document):\n",
        "    # We are tokenizing using the PunktSentenceTokenizer\n",
        "    # we call an instance of this class as sentence_tokenizer\n",
        "    doc_tokenizer = PunktSentenceTokenizer()\n",
        "    \n",
        "    # tokenize() method: takes our document as input and returns a list of all the sentences in the document\n",
        "    \n",
        "    # sentences is a list containing each sentence of the document as an element\n",
        "    sentences_list = doc_tokenizer.tokenize(document)\n",
        "    return sentences_list"
      ],
      "metadata": {
        "id": "do0u8cjwNgLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading a file and \n",
        "# printing the size of the file\n",
        "document = readDoc()\n",
        "print('The length of the file is:', end=' ')\n",
        "print(len(document))"
      ],
      "metadata": {
        "id": "blHykQHgNgPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we want to tokenize the document for further processing\n",
        "# tokenizing the sentence means that we are creating a list of all the sentences of the document.\n",
        "# Need of tokenizing the document: Initially the document is in just a string format.\n",
        "# if we want to process the document, we need to store it in a data structure.\n",
        "# Tokenization of document into words is also possible, but we will go with the tokenizing with the sentences\n",
        "# Since we want to choose the most relevant sentences, we need to generate tokens of sentences only\n",
        "sentences_list = tokenize(document)\n",
        "\n",
        "# let us print the size of memory used by the list sentences\n",
        "print('The size of the list in Bytes is: {}'.format(sys.getsizeof(sentences_list)))\n",
        "\n",
        "# the size of one of the element of the list\n",
        "print('The size of the item 0 in Bytes is: {}'.format(sys.getsizeof(sentences_list[0])))"
      ],
      "metadata": {
        "id": "ofmrCNGhNgTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let us see the data type of sentences_list\n",
        "# It will be list\n",
        "print(type(sentences_list))"
      ],
      "metadata": {
        "id": "y99CXHZ5NgXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let us analyse the elements of the sentences\n",
        "# len() method applies on the list and provides the number of elements in the list\n",
        "print('The size of the list \"sentences\" is: {}'.format(len(sentences_list)))"
      ],
      "metadata": {
        "id": "LkntlFdiNubN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the elements of the list\n",
        "# If the input document is long, which on realistically will be wrong, we would not like to print the entire document\n",
        "for i in sentences_list:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "63ZcvuVZNugE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert a collection of text documents to a matrix of token counts\n",
        "# fit_transform method of CountVectorizer() class \n",
        "# Learn the vocabulary dictionary and return term-document matrix. \n",
        "# I/p: An iterable which yields either str, unicode or file objects.\n",
        "# O/p: The term-document matrix named cv_matrix\n",
        "cv = CountVectorizer()\n",
        "cv_matrix = cv.fit_transform(sentences_list)"
      ],
      "metadata": {
        "id": "BdkFRo-lNui3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the cv_matrix type\n",
        "# and how it is being stored in memory?\n",
        "# it is stored in the compressed row format\n",
        "# compressed row format: \n",
        "print('The data type of bow matrix {}'.format(type(cv_matrix)))\n",
        "print('Shape of the matrix {}'.format(cv_matrix.get_shape))\n",
        "print('Size of the matrix is: {}'.format(sys.getsizeof(cv_matrix)))\n",
        "print(cv.get_feature_names())\n",
        "print(cv_matrix.toarray())"
      ],
      "metadata": {
        "id": "gjjgZ-jmNumK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tnormalized: document-term matrix normalized (value 0-1) according to the TF-IDF\n",
        "# TF(Term Frequency): the no. of times a term(a word here) appears in the current document(single sentence here)\n",
        "# IDF(Inverse Document Frequency): the no. of times a term(a word here) appears in the entire corpus\n",
        "# Corpus: set of all sentences\n",
        "\n",
        "normal_matrix = TfidfTransformer().fit_transform(cv_matrix)\n",
        "print(normal_matrix.toarray())"
      ],
      "metadata": {
        "id": "AH0hMqJwN7g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(normal_matrix.T.toarray)\n",
        "res_graph = normal_matrix * normal_matrix.T\n",
        "# plt.spy(res_graph)"
      ],
      "metadata": {
        "id": "AK71sACSN7lR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drawing a graph to proceed for the textrank algorithm\n",
        "# nx_graph is a graph developed using the networkx library\n",
        "# each node represents a sentence\n",
        "# an edge represents that they have words in common\n",
        "# the edge weight is the number of words that are common in both of the sentences(nodes)\n",
        "# nx.draw() method is used to draw the graph created\n",
        "\n",
        "nx_graph = nx.from_scipy_sparse_matrix(res_graph)\n",
        "nx.draw_circular(nx_graph)\n",
        "print('Number of edges {}'.format(nx_graph.number_of_edges()))\n",
        "print('Number of vertices {}'.format(nx_graph.number_of_nodes()))\n",
        "plt.show()\n",
        "print('The memory used by the graph in Bytes is: {}'.format(sys.getsizeof(nx_graph)))"
      ],
      "metadata": {
        "id": "pepr-CNyN7qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ranks is a dictionary with key=node(sentences) and value=textrank (the rank of each of the sentences)\n",
        "ranks = nx.pagerank(nx_graph)\n",
        "\n",
        "# analyse the data type of ranks\n",
        "print(type(ranks))\n",
        "print('The size used by the dictionary in Bytes is: {}'.format(sys.getsizeof(ranks)))\n",
        "\n",
        "# print the dictionary\n",
        "for i in ranks:\n",
        "    print(i, ranks[i])"
      ],
      "metadata": {
        "id": "OPlmyMo3N7uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# enumerate method: returns an enumerate object\n",
        "# Use of list Comprehensions\n",
        "# O/p: sentence_array is the sorted(descending order w.r.t. score value) 2-d array of ranks[sentence] and sentence \n",
        "# For example, if there are two sentences: S1 (with a score of S1 = s1) and S2 with score s2, with s2>s1\n",
        "# then sentence_array is [[s2, S2], [s1, S1]]\n",
        "sentence_array = sorted(((ranks[i], s) for i, s in enumerate(sentences_list)), reverse=True)\n",
        "sentence_array = np.asarray(sentence_array)"
      ],
      "metadata": {
        "id": "kymG2SyYOGhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# as sentence_array is in descending order wrt score value\n",
        "# fmax is the largest score value(the score of first element)\n",
        "# fmin is the smallest score value(the score of last element)\n",
        "\n",
        "rank_max = float(sentence_array[0][0])\n",
        "rank_min = float(sentence_array[len(sentence_array) - 1][0])"
      ],
      "metadata": {
        "id": "2IDq-Es2OGlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the largest and smallest value of scores of the sentence\n",
        "print(rank_max)\n",
        "print(rank_min"
      ],
      "metadata": {
        "id": "UPcY2z7XOKiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization of the scores\n",
        "# so that it comes out in the range 0-1\n",
        "# fmax becomes 1\n",
        "# fmin becomes 0\n",
        "# store the normalized values in the list temp_array\n",
        "\n",
        "temp_array = []\n",
        "\n",
        "# if all sentences have equal ranks, means they are all the same\n",
        "# taking any sentence will give the summary, say the first sentence\n",
        "flag = 0\n",
        "if rank_max - rank_min == 0:\n",
        "    temp_array.append(0)\n",
        "    flag = 1\n",
        "\n",
        "# If the sentence has different ranks\n",
        "if flag != 1:\n",
        "    for i in range(0, len(sentence_array)):\n",
        "        temp_array.append((float(sentence_array[i][0]) - rank_min) / (rank_max - rank_min))\n",
        "\n",
        "print(len(temp_array))"
      ],
      "metadata": {
        "id": "I7Gis3rXOKqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculation of threshold:\n",
        "# We take the mean value of normalized scores\n",
        "# any sentence with the normalized score 0.2 more than the mean value is considered to be \n",
        "threshold = (sum(temp_array) / len(temp_array)) + 0.2"
      ],
      "metadata": {
        "id": "jKlh710hORcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate out the sentences that satiasfy the criteria of having a score above the threshold\n",
        "sentence_list = []\n",
        "if len(temp_array) > 1:\n",
        "    for i in range(0, len(temp_array)):\n",
        "        if temp_array[i] > threshold:\n",
        "                sentence_list.append(sentence_array[i][1])\n",
        "else:\n",
        "    sentence_list.append(sentence_array[0][1])"
      ],
      "metadata": {
        "id": "NIG55NMGNupv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = sentence_list"
      ],
      "metadata": {
        "id": "jO78NxY-OWKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(sentence_list)\n",
        "summary = \" \".join(str(x) for x in sentence_list)\n",
        "print(summary)\n",
        "# save the data in another file, names sum.txt\n",
        "f = open('final3.txt', 'a+')\n",
        "#print(type(f))\n",
        "f.write('\\n')\n",
        "f.write(summary)\n",
        "f.close"
      ],
      "metadata": {
        "id": "V-AP2iEpOYF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for lines in sentence_list:\n",
        "    print(lines)"
      ],
      "metadata": {
        "id": "6lfZiu4OOdi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATION USING ROUGE"
      ],
      "metadata": {
        "id": "L3ZuY18EOhr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "id": "qbGOHV4dOmRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pythonrouge"
      ],
      "metadata": {
        "id": "dTirtuNxOxUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_out = ['Traveling is one of the most valuable and enriching experiences in our lives increasing our knowledge, opening our minds, enriching our spirits, extending new moral  values; and you can find many destinations in the world that would fulfill your desires. But, if what you are looking for is a unique and memorable experience that will give you  the opportunity to learn about another culture through art, cultural events, historical cites,  and archeological treasures, a place to visit is certainly Italy. Among many places in the  world  where you can relive the past, Italy is certainly the most  charming placeThis little country in the middle of the Mediterranean represents the cradle of the Roman Empire  and the Renaissance, and offers an incredible number of opportunities for  people eager to learn about an ancient civilization that shapedthe history of many cultures. Moreover, United Nations Educational, Scientific and Cultural']\n",
        "reference =[document]\n",
        "rouge = Rouge()\n",
        "rouge.get_scores(model_out,reference)"
      ],
      "metadata": {
        "id": "vO44wdbWO1DD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}